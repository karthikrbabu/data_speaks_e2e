{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "earlier-bibliography",
   "metadata": {},
   "source": [
    "# T5 Model Base Implementation\n",
    "\n",
    "The purpose of this notebook is to demonstrate training using tensorflow 2 and keras. This notebook includes tf Data pipelines for build any other NLP task in a text to text fashion. Anyone can adapt the data pipeline to thier own datasets. Uses the efficient [Datasets](https://github.com/huggingface/datasets) from ðŸ¤— as source for training.\n",
    "\n",
    "#### Features:\n",
    "- Train TF T5 on E2E Cleaned Data to Text Problem\n",
    "- Train T5 using keras trainer fucntion\n",
    "- tf.Data pipeline\n",
    "- [Datasets from ðŸ¤—](https://github.com/huggingface/datasets) as source\n",
    "- Log metrics using tensorboard\n",
    "- Profile your experiment with the brand new tensorflow profiler !!\n",
    "\n",
    "#### Steps:\n",
    "* Import Libraries\n",
    "* Load Train/Dev/Test Data\n",
    "* Config Definitions\n",
    "* Pre-process Data (Tensors)\n",
    "* Quick Tensor EDA\n",
    "* Tensorboard Loading\n",
    "* Optimizer Init\n",
    "* Train Model\n",
    "* Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handy-group",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#Utilities\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import math\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "import collections\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "\n",
    "#NLTK \n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "#HuggingFace\n",
    "import transformers\n",
    "from transformers import (TFAutoModelWithLMHead, AutoTokenizer,\n",
    "                            TFTrainer, TFTrainingArguments, T5Tokenizer, TFT5ForConditionalGeneration,\n",
    "                            TFT5Model, T5Config, pipeline)\n",
    "import datasets\n",
    "from datasets import load_dataset, list_datasets\n",
    "\n",
    "# Tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "#AWS\n",
    "import boto3\n",
    "s3 = boto3.resource('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "israeli-catalog",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_dir = os.getcwd()\n",
    "print(\"Experiment Dir: \", exp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developed-pound",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = os.path.abspath(os.path.join(os.getcwd(),os.pardir, os.pardir, os.pardir))\n",
    "os.chdir(base_dir)\n",
    "print(\"Base Dir: \", base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "close-burner",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Custom Utils Lib\n",
    "from src.utils.utils import (get_model_output, write_model_output, save_metrics,\n",
    "                         encode, to_tf_dataset, create_dataset, compute_metrics, save_model_to_s3)\n",
    "from src.classes.t5Wrapper import T5Wrapper\n",
    "from src.classes.customScheduler import CustomSchedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "magnetic-pavilion",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_version = tf.__version__\n",
    "print(\"Tensorflow: \", tf_version)\n",
    "print(\"Transformers: \", transformers.__version__)\n",
    "print(\"Datasets: \", datasets.__version__)\n",
    "\n",
    "tf_version_split = tf_version.split('.')\n",
    "assert int(tf_version_split[0])==2 and int(tf_version_split[-2])>=3, f\"Tensorflow version should be '2.3+,x', given {tf_version}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "signal-message",
   "metadata": {},
   "source": [
    "### Setup Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lasting-productivity",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls {base_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "color-hybrid",
   "metadata": {},
   "outputs": [],
   "source": [
    "#AWS box path we should keep\n",
    "tb_data_dir = f\"{exp_dir}/tf_data\"\n",
    "log_dir = f\"{tb_data_dir}/experiments/t5/logs\"\n",
    "save_path = f\"{tb_data_dir}/experiments/t5/models\"\n",
    "cache_path_train = f\"{tb_data_dir}/cache/t5.train\"\n",
    "cache_path_test = f\"{tb_data_dir}/cache/t5.test\"\n",
    "\n",
    "print(\"Experiment Base directory: \",exp_dir)\n",
    "model_path = f'{exp_dir}/model'\n",
    "print('model_path: ', model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "limiting-forestry",
   "metadata": {},
   "source": [
    "### Init Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "successful-portuguese",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('t5-small')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portuguese-debate",
   "metadata": {},
   "source": [
    "### Process Train/ Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chief-insurance",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = load_dataset('e2e_nlg_cleaned', split='train')\n",
    "validation = load_dataset('e2e_nlg_cleaned', split='validation')\n",
    "\n",
    "train.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "starting-instruction",
   "metadata": {},
   "outputs": [],
   "source": [
    "def swapEqualSign(record):\n",
    "    mr=record[\"meaning_representation\"]\n",
    "    new_mr = mr.replace(\"[\", \"=[\")\n",
    "    record[\"meaning_representation\"] = new_mr\n",
    "    return record\n",
    "\n",
    "def add_special_tokens(record):\n",
    "    new_mr=\"\"\n",
    "    tag_pairs = record['meaning_representation'].split(',')\n",
    "    for i, pair in enumerate(tag_pairs):\n",
    "        if i == (len(tag_pairs)-1):\n",
    "            tag_name, value = pair.split('=')\n",
    "            tag_name = tag_name.strip()\n",
    "            new_mr += f\"<{tag_name}> tag_name={value}\"\n",
    "        else:\n",
    "            tag_name, value = pair.split('=')\n",
    "            tag_name = tag_name.strip()\n",
    "            new_mr += f\"<{tag_name}> {tag_name}={value}, \"\n",
    "\n",
    "    record[\"meaning_representation\"] = new_mr\n",
    "    return record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "senior-consolidation",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "train = train.map(lambda x: swapEqualSign(x))\n",
    "validation=validation.map(lambda x: swapEqualSign(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "editorial-price",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.map(lambda x: add_special_tokens(x))\n",
    "validation=validation.map(lambda x: add_special_tokens(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cordless-sound",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iter(train))\n",
    "print(\"Example data from the dataset: \\n\", data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intimate-companion",
   "metadata": {},
   "source": [
    "### Init Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lined-startup",
   "metadata": {},
   "outputs": [],
   "source": [
    "warmup_steps = 1e4\n",
    "epochs = 5\n",
    "batch_size = 30\n",
    "encoder_max_len = 60\n",
    "decoder_max_len = 60\n",
    "buffer_size = 1000\n",
    "ntrain = len(train)\n",
    "nvalid = len(validation)\n",
    "steps = int((ntrain//epochs)// batch_size)\n",
    "valid_steps = int((nvalid//epochs)// batch_size)\n",
    "\n",
    "print(\"Train Data Length: \", ntrain)\n",
    "print(\"Validation Data Length: \", nvalid)\n",
    "print(\"Total Steps: \", steps)\n",
    "print(\"Total Validation Steps: \", valid_steps)\n",
    "print(\"Batch Size: \", batch_size)\n",
    "print(\"Total Epochs: \", epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "useful-square",
   "metadata": {},
   "source": [
    "## Data Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opening-table",
   "metadata": {},
   "source": [
    "### Process Train/Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serious-preference",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train.map(lambda x: encode(x, tokenizer))\n",
    "valid_ds = validation.map(lambda x: encode(x, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aboriginal-jacket",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dying-stack",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = next(iter(train_ds))\n",
    "print(\"Example data from the mapped dataset: \\n\", ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amended-sport",
   "metadata": {},
   "source": [
    "### Process Train/Validation =>  Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "looking-shakespeare",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_train_ds = to_tf_dataset(train_ds)\n",
    "tf_valid_ds = to_tf_dataset(valid_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jewish-robertson",
   "metadata": {},
   "source": [
    "### Build Train/ Validation =>  Model Ready Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reflected-outdoors",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_train_ds= create_dataset(tf_train_ds, batch_size=batch_size, \n",
    "                         shuffling=True, cache_path = None)\n",
    "tf_valid_ds = create_dataset(tf_valid_ds, batch_size=batch_size, \n",
    "                         shuffling=False, cache_path = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "primary-therapy",
   "metadata": {},
   "source": [
    "### Custom Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "essential-commerce",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#Example\n",
    "plt.style.use('ggplot')\n",
    "schedule = CustomSchedule()\n",
    "plt.plot(schedule(tf.range(25000, dtype=tf.float32)))\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Learning rate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capital-marketplace",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "written-aurora",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [tf.keras.metrics.SparseTopKCategoricalAccuracy(name='accuracy') ]\n",
    "learning_rate = CustomSchedule() # learning_rate = 0.001  # Instead set a static learning rate\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "model = T5Wrapper.from_pretrained('t5-small')\n",
    "\n",
    "model.compile(optimizer=optimizer, metrics=metrics)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suffering-ideal",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit the Model\n",
    "epochs_done = 0\n",
    "model.fit(tf_train_ds, epochs=epochs, steps_per_epoch=steps,\n",
    "          validation_data=tf_valid_ds, validation_steps=valid_steps, initial_epoch=epochs_done)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brutal-denver",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intensive-guidance",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reusing the model, just adjusting the experiment\n",
    "model_path = '/home/ubuntu/karthik/data_speaks_e2e/src/experiments/gen_experiments/model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressed-separation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model\n",
    "loaded_model = T5Wrapper.from_pretrained(model_path) #to be uncommented when required. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesbian-bobby",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diverse-arrow",
   "metadata": {},
   "source": [
    "### Generate Results + Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southern-orientation",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_array = []\n",
    "\n",
    "param1 = {'max_length': 50, \n",
    "          'min_length': 10, \n",
    "          'early_stopping': True, \n",
    "          'do_sample': True, \n",
    "          'temperature': 0.1, \n",
    "          'top_k': 0, \n",
    "          'no_repeat_ngram_size': 2\n",
    "         }\n",
    "\n",
    "param2 = {'max_length': 50, \n",
    "          'min_length': 10, \n",
    "          'early_stopping': True, \n",
    "          'do_sample': False, \n",
    "          'num_beams': 3, \n",
    "          'no_repeat_ngram_size': 2\n",
    "         }\n",
    "\n",
    "\n",
    "param3 = {'max_length': 50, \n",
    "          'min_length': 10, \n",
    "          'early_stopping': True, \n",
    "          'do_sample': False, \n",
    "          'num_beams': 5, \n",
    "          'no_repeat_ngram_size': 2\n",
    "         }\n",
    "\n",
    "\n",
    "param4 = {'max_length': 50, \n",
    "          'min_length': 10, \n",
    "          'early_stopping': True, \n",
    "          'do_sample': False, \n",
    "          'num_beams': 7, \n",
    "          'no_repeat_ngram_size': 2\n",
    "         }\n",
    "\n",
    "param5 = {'max_length': 50, \n",
    "          'min_length': 10, \n",
    "          'early_stopping': True, \n",
    "          'do_sample': True, \n",
    "          'top_p': 0.35,\n",
    "          'top_k': 0, \n",
    "          'no_repeat_ngram_size': 2\n",
    "         }\n",
    "\n",
    "\n",
    "param6 = {'max_length': 45, \n",
    "          'min_length': 10, \n",
    "          'early_stopping': True, \n",
    "          'do_sample': True, \n",
    "          'temperature': 0.1, \n",
    "          'top_k': 0, \n",
    "          'no_repeat_ngram_size': 2\n",
    "         }\n",
    "\n",
    "param7 = {'max_length': 45, \n",
    "          'min_length': 10, \n",
    "          'early_stopping': True, \n",
    "          'do_sample': False, \n",
    "          'num_beams': 3, \n",
    "          'no_repeat_ngram_size': 2\n",
    "         }\n",
    "\n",
    "\n",
    "param8 = {'max_length': 45, \n",
    "          'min_length': 10, \n",
    "          'early_stopping': True, \n",
    "          'do_sample': False, \n",
    "          'num_beams': 5, \n",
    "          'no_repeat_ngram_size': 2\n",
    "         }\n",
    "\n",
    "\n",
    "param9 = {'max_length': 45, \n",
    "          'min_length': 10, \n",
    "          'early_stopping': True, \n",
    "          'do_sample': False, \n",
    "          'num_beams': 7, \n",
    "          'no_repeat_ngram_size': 2\n",
    "         }\n",
    "\n",
    "param10 = {'max_length': 45, \n",
    "          'min_length': 10, \n",
    "          'early_stopping': True, \n",
    "          'do_sample': True, \n",
    "          'top_p': 0.35,\n",
    "          'top_k': 0, \n",
    "          'no_repeat_ngram_size': 2\n",
    "         }\n",
    "\n",
    "\n",
    "\n",
    "params_array.append(param1)\n",
    "params_array.append(param2)\n",
    "params_array.append(param3)\n",
    "params_array.append(param4)\n",
    "params_array.append(param5)\n",
    "params_array.append(param6)\n",
    "params_array.append(param7)\n",
    "params_array.append(param8)\n",
    "params_array.append(param9)\n",
    "params_array.append(param10)\n",
    "\n",
    "\n",
    "# max_length (int, optional, defaults to 20) â€“ The maximum length of the sequence to be generated.\n",
    "\n",
    "# min_length (int, optional, defaults to 10) â€“ The minimum length of the sequence to be generated.\n",
    "\n",
    "# do_sample (bool, optional, defaults to False) â€“ Whether or not to use sampling ; use greedy decoding otherwise.\n",
    "\n",
    "# early_stopping (bool, optional, defaults to False) â€“ Whether to stop the beam search when at least num_beams sentences are finished per batch or not.\n",
    "\n",
    "# num_beams (int, optional, defaults to 1) â€“ Number of beams for beam search. 1 means no beam search.\n",
    "\n",
    "# temperature (float, optional, defaults tp 1.0) â€“ The value used to module the next token probabilities.\n",
    "\n",
    "# top_k (int, optional, defaults to 50) â€“ The number of highest probability vocabulary tokens to keep for top-k-filtering.\n",
    "\n",
    "# top_p (float, optional, defaults to 1.0) â€“ If set to float < 1, only the most probable tokens with probabilities that add up to top_p or higher are kept for generation.\n",
    "\n",
    "# repetition_penalty (float, optional, defaults to 1.0) â€“ The parameter for repetition penalty. 1.0 means no penalty. See this paper for more details.\n",
    "\n",
    "# length_penalty (float, optional, defaults to 1.0) â€“ Exponential penalty to the length. 1.0 means no penalty.\n",
    "# Set to values < 1.0 in order to encourage the model to generate shorter sequences, to a value > 1.0 in order to encourage the model to produce longer sequences.\n",
    "\n",
    "# no_repeat_ngram_size (int, optional, defaults to 0) â€“ If set to int > 0, all ngrams of that size can only occur once.\n",
    "\n",
    "# num_return_sequences (int, optional, defaults to 1) â€“ The number of independently computed returned sequences for each element in the batch.\n",
    "\n",
    "# use_cache â€“ (bool, optional, defaults to True): Whether or not the model should use the past last key/values attentions (if applicable to the model) to speed up decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quick-franchise",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write model outputs\n",
    "for param_set in params_array:\n",
    "\n",
    "    #Returns a list of all the model generated outputs\n",
    "    model_ouput = get_model_output(loaded_model, tokenizer, param_set, None, tf_valid_ds, None)\n",
    "\n",
    "    v_out = model_ouput['validation']['output']\n",
    "    ts_val=time.strftime(\"%Y%m%d_%H%M\")\n",
    "    print(ts_val)\n",
    "    write_model_output(valid_ds, \"validation\", ts_val, v_out, write_path=exp_dir)\n",
    "    \n",
    "    # Let's Use E2E Evaluation Metrics\n",
    "    scores = compute_metrics(exp_dir, base_dir, ts_val, 'validation', param_set)\n",
    "\n",
    "    print(scores)\n",
    "    \n",
    "    save_metrics(exp_dir, ts_val, scores)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imported-killer",
   "metadata": {},
   "source": [
    "#### If we like the scores and want to save the scores to our model track\n",
    "(We should probably club this with when we save to S3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hearing-nutrition",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### Save Model (only if its worth it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eastern-bleeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep for AWS path\n",
    "model.save_pretrained(f'{model_path}')\n",
    "# save_model_to_s3(model,base_dir, ts_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norman-latvia",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annual-skirt",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "#Below is an optional step to load a pre-trained and saved model to directly run predictions.\n",
    "\n",
    "#model = T5Wrapper.from_pretrained(model_path) #to be uncommented when required. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desperate-cooperative",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
